{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 1 (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling (8 marks)\n",
    "\n",
    "In this assessment you implement an algorithm that reads a csv data dump from Reddit and creates a database (relational or non-relational), taking into account the different entities and relationships holding between them. With this database in place, you are also asked to implement queries for generating reports about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter code for loading the csv file and connecting to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data_portfolio_21.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the school's MySQL server using your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import credentials\n",
    "\n",
    "password = credentials.MYSQL_PASSWORD\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host=credentials.HOST_NAME,\n",
    "                             user=credentials.MYSQL_USERNAME,\n",
    "                             password=password,\n",
    "                             db=credentials.DB_NAME,\n",
    "                             charset='utf8mb4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and Python code for creating the tables [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "try:\n",
    "    with connection.cursor() as cur:\n",
    "        q=\"\"\"ALTER DATABASE c2075016_covid_reddit CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\"\"\"\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "        \n",
    "        q=\"\"\"DROP TABLE IF EXISTS favourites\"\"\"\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "        \n",
    "        q=\"\"\"DROP TABLE IF EXISTS posts\"\"\"\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "        \n",
    "        q=\"\"\"DROP TABLE IF EXISTS users\"\"\"\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "        \n",
    "        q=\"\"\"DROP TABLE IF EXISTS subreddits\"\"\"\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "        \n",
    "        q= \"\"\"CREATE TABLE subreddits (\n",
    "        subr_ID INT AUTO_INCREMENT NOT NULL,\n",
    "        subr_name VARCHAR(64) NOT NULL,\n",
    "        subr_created_at DATE NOT NULL,\n",
    "        subr_description VARCHAR(4096),\n",
    "        subr_numb_members INT UNSIGNED NOT NULL,\n",
    "        subr_numb_posts INT UNSIGNED NOT NULL,\n",
    "        CONSTRAINT subreddits_PK PRIMARY KEY (subr_ID)\n",
    "        ) DEFAULT CHARSET utf8mb4 COLLATE utf8mb4_unicode_ci;\"\"\"\n",
    "# I chose to use DEFAULT CHARSET utf8mb4 COLLATE utf8mb4_unicode_ci for every table in order to allow non-ASCII \n",
    "# characters in the database such as emojis and special characters. I chose to do this rather than cleaning the \n",
    "# text data to remove non-ASCII characters in order to avoid the data loss that this would of necessity involve.\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "        \n",
    "        q=\"\"\"CREATE TABLE users(\n",
    "        user_ID INT AUTO_INCREMENT NOT NULL,\n",
    "        user_name VARCHAR(128) NOT NULL,\n",
    "        user_num_posts INT UNSIGNED,\n",
    "        user_registered_at DATE,\n",
    "        user_upvote_ratio FLOAT,\n",
    "        CONSTRAINT users_PK PRIMARY KEY (user_ID)\n",
    "        ) DEFAULT CHARSET utf8mb4 COLLATE utf8mb4_unicode_ci;\"\"\"\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "        \n",
    "        q=\"\"\"CREATE TABLE posts(\n",
    "        post_ID INT AUTO_INCREMENT NOT NULL,\n",
    "        author_ID INT NOT NULL,\n",
    "        subreddit_ID INT NOT NULL,\n",
    "        posted_at DATE NOT NULL,\n",
    "        num_comments INT UNSIGNED NOT NULL,\n",
    "        score INT UNSIGNED NOT NULL,\n",
    "        selftext TEXT NOT NULL,\n",
    "        title TEXT NOT NULL,\n",
    "        total_awards INT UNSIGNED NOT NULL,\n",
    "        upvote_ratio FLOAT NOT NULL,\n",
    "        CONSTRAINT posts_PK PRIMARY KEY (post_ID),\n",
    "        CONSTRAINT authors_FK FOREIGN KEY (author_ID) REFERENCES users(user_ID) ON DELETE CASCADE,\n",
    "        CONSTRAINT subreddits_FK FOREIGN KEY (subreddit_ID) REFERENCES subreddits(subr_ID) ON DELETE CASCADE\n",
    "        ) DEFAULT CHARSET utf8mb4 COLLATE utf8mb4_unicode_ci;\"\"\"\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "# I used the TEXT data type for the post title and selftext beacuse the in some posts the combined length of the \n",
    "# title and description exceeded the row length limit of 65535 bytes. Therefore it was not possible to balance the \n",
    "# sizes of varchar fields to include all the data. Therefore, to prevent data loss, the TEXT data type was used, \n",
    "# which avoids this problem by storing the data from these fields separately, with only 9 to 12 bytes being \n",
    "# contributes towards the row length limit per TEXT field. \n",
    "\n",
    "# Missing data in the selftext of posts was dealt with by setting the selftext value to an empty string if NULL. I \n",
    "# chose this appraoch rather than inserting nulls into the database because I wished to concatenate the title and \n",
    "# selftext of posts later in order to perform the classification experiment, and concatenating with a null value \n",
    "# returns a null. For the same reason, the selftext and title columns were set NOT NULL to prevent title or \n",
    "# selftext containing nulls being inserted into the database in the future. \n",
    "# [REF: https://www.w3schools.com/mysql/func_mysql_concat.asp Accessed on: 05/05/2021] \n",
    "# Furthermore, this approach allowed the posts to be inserted into the database in one cur.executemany call rather \n",
    "# than having to distinguish between those that had NULL selftext and those that did not.\n",
    "        \n",
    "        q=\"\"\"CREATE TABLE favourites (\n",
    "        subr_ID INT NOT NULL,\n",
    "        user_ID INT NOT NULL,\n",
    "        CONSTRAINT favourites_FK PRIMARY KEY (subr_ID, user_ID),\n",
    "        CONSTRAINT favourited_subreddits_FK FOREIGN KEY (subr_ID) REFERENCES subreddits(subr_ID) ON DELETE CASCADE,\n",
    "        CONSTRAINT favouriting_user_FK FOREIGN KEY (user_ID) REFERENCES users(user_ID) ON DELETE CASCADE\n",
    "        ) DEFAULT CHARSET utf8mb4 COLLATE utf8mb4_unicode_ci;\"\"\"\n",
    "        cur.execute(q)\n",
    "        connection.commit()\n",
    "        \n",
    "        \n",
    "        print('success')\n",
    "        \n",
    "finally: \n",
    "    connection.close\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python logic for reading in the data [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "seen_users={}\n",
    "seen_subreddits={}\n",
    "authors_list=[]\n",
    "subreddits_list=[]\n",
    "favourites_list=[]\n",
    "user_ID=1\n",
    "subreddit_ID=1\n",
    "posts_list=[]\n",
    "favourites_dictionary={}\n",
    "\n",
    "i=1\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    \n",
    "    csv_reader=csv.DictReader(f, delimiter=',', quotechar='\"')\n",
    "    for line_dictionary in csv_reader:\n",
    "\n",
    "        if line_dictionary['subr_faved_by']=='[]':\n",
    "            line_dictionary['subr_faved_by']=[]\n",
    "        else:\n",
    "            line_dictionary['subr_faved_by']=line_dictionary['subr_faved_by'][2:-2].split('\\', \\'')\n",
    "        \n",
    "        author=line_dictionary[\"author\"]\n",
    "\n",
    "        if author not in seen_users:\n",
    "\n",
    "            user_num_posts=line_dictionary[\"user_num_posts\"]\n",
    "            user_registered_at=line_dictionary[\"user_registered_at\"]\n",
    "            user_upvote_ratio=line_dictionary[\"user_upvote_ratio\"]\n",
    "\n",
    "            author_details=[user_ID, author, user_num_posts, user_registered_at, user_upvote_ratio]\n",
    "            authors_list.append(author_details)\n",
    "\n",
    "            seen_users[author]=user_ID\n",
    "            user_ID+=1\n",
    "\n",
    "        subreddit=line_dictionary[\"subreddit\"]\n",
    "\n",
    "        if subreddit not in seen_subreddits:\n",
    "\n",
    "            subr_created_at=line_dictionary[\"subr_created_at\"]\n",
    "            subr_description=line_dictionary[\"subr_description\"]\n",
    "            subr_numb_members=line_dictionary[\"subr_numb_members\"]\n",
    "            subr_numb_posts=line_dictionary[\"subr_numb_posts\"]\n",
    "\n",
    "            subreddit_details=[subreddit_ID, subreddit, subr_created_at, subr_description, subr_numb_members, subr_numb_posts]\n",
    "            subreddits_list.append(subreddit_details)\n",
    "            \n",
    "            favourited_users_list=line_dictionary[\"subr_faved_by\"]\n",
    "            favourites_dictionary[subreddit_ID]=favourited_users_list\n",
    "\n",
    "            seen_subreddits[subreddit]=subreddit_ID\n",
    "            subreddit_ID+=1\n",
    "            \n",
    "\n",
    "\n",
    "        this_author_ID=seen_users[author]\n",
    "        this_subreddit_ID=seen_subreddits[subreddit]\n",
    "        posted_at=line_dictionary[\"posted_at\"]\n",
    "        num_comments=line_dictionary[\"num_comments\"]\n",
    "        score=line_dictionary[\"score\"]\n",
    "        if line_dictionary[\"selftext\"]==\"NULL\":\n",
    "            selftext=''\n",
    "        else:\n",
    "            selftext=line_dictionary[\"selftext\"]\n",
    "        title=line_dictionary[\"title\"]\n",
    "        total_awards_received=line_dictionary[\"total_awards_received\"]\n",
    "        upvote_ratio=line_dictionary[\"upvote_ratio\"]\n",
    "\n",
    "        posts_details=[i, this_author_ID, this_subreddit_ID, posted_at, num_comments, score, selftext, title, total_awards_received, upvote_ratio]\n",
    "        posts_list.append(posts_details)\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    for subreddit_ID in favourites_dictionary:\n",
    "        for user in favourites_dictionary[subreddit_ID]:\n",
    "            favouriting_user_ID=seen_users[user]\n",
    "            favourites_details=[subreddit_ID, favouriting_user_ID]\n",
    "            favourites_list.append(favourites_details) \n",
    "        \n",
    "    print(\"Success\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only potential multi-valued column present in this dataset was the 'subr_faved_by' data which contains a stringified list of users. I dealt with this by first extracting the list of strings from the stringified list by removing the sqaure brackets and splitting with '\\', \\'' as the delimiter. On the first pass through the data, these lists were saved for unique subreddits in the variable favourites_dictionary, with the ubreddit_ID as the key. Then on a second pass over this data, for each subreddit, the user_ID was obtained for each element of the favourites list and each pair \\[subreddit_ID, user_ID\\] appended to the list of favourites for insertion into a dedicated favourites linking table in the database.\n",
    "\n",
    "It was initially thought that there may be some users present in the dataset as having favourited a subreddit, but not having posted, and therefore that their full details might not be present. However, running \n",
    "                \n",
    "            if user not in seen_users:\n",
    "                seen_users[user]=user_ID\n",
    "                user_details=[user_ID, user]\n",
    "                users_list.append(user_details)\n",
    "was found not to increase the length of users_list, so this step was omitted. However, the possibility for this to be part of any future dataset that may be added to the databse was left open by allowing all fields in the user table apart from 'user_ID' and 'user_name' to take a NULL value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and Python code for populating the tables [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with connection.cursor() as cur:\n",
    "        q=\"\"\"INSERT INTO subreddits VALUES (%s, %s, %s, %s, %s, %s);\"\"\"\n",
    "        cur.executemany(q, subreddits_list)\n",
    "        connection.commit()\n",
    "        \n",
    "        q=\"\"\"INSERT INTO users VALUES (%s, %s, %s, %s, %s);\"\"\"\n",
    "        cur.executemany(q, authors_list)\n",
    "        connection.commit()\n",
    "    \n",
    "        q=\"\"\"INSERT INTO posts VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"\"\"\n",
    "        cur.executemany(q, posts_list)\n",
    "        connection.commit()\n",
    "    \n",
    "        q=\"\"\"INSERT INTO favourites VALUES (%s, %s);\"\"\"\n",
    "        cur.executemany(q, favourites_list)\n",
    "        connection.commit()\n",
    "    \n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
